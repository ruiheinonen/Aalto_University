{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d47e53b1c68453da5a418482108b42f",
     "grade": false,
     "grade_id": "cell-79acac02b7851909",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2020)\n",
    "Pekka Marttinen, Santosh Hiremath, Marko J채rvenp채채, Tianyu Cui, Yogesh Kumar, Diego Mesquita, Zheyang Shen, Alexander Aushev, Khaoula El Mekkaoui, Joakim J채rvinen.\n",
    "\n",
    "## Assignment 5, due on Tuesday,  18th February at 23:55.\n",
    "\n",
    "\n",
    "### Contents\n",
    "1. [Problem 1: EM for missing observations](#Problem-1:-EM-for-missing-observations)\n",
    "2. [Problem 2: Extension of 'simple example' from the lecture](#Problem-2:-Extension-of-'simple-example'-from-the-lecture)\n",
    "3. [Problem 3: PyTorch](#Problem-3:-PyTorch)  \n",
    "\n",
    "\n",
    "# Problem 1: EM for missing observations\n",
    "Suppose random variables $X_{i}$ follow a bivariate normal distribution $X_{i}\\sim \\mathcal{N}_{2}(0,\\Sigma)$, where\n",
    "$ \\Sigma = \\begin{bmatrix} 1 & \\rho\\\\ \\rho & 1 \\end{bmatrix} $.\n",
    "\n",
    "Suppose further that we have observations on $X_{1}=(X_{11},X_{12})^{T}$, $X_{2}=(X_{21},X_{22})^{T}$ and $X_{3}=(X_{31},X_{32})^{T}$, such that\n",
    "$X_{1}$ and $X_{3}$ are fully observed, and from $X_{2}$ we have observed only\n",
    "the second coordinate. Thus, our data matrix can be written as\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12}\\\\\n",
    "? & x_{22}\\\\\n",
    "x_{31} & x_{32}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "where the rows correspond to the transposed observations $\\mathbf{x}_{1}^{T},\\mathbf{x}_{2}^{T},\\mathbf{x}_{3}^{T}$. Suppose we want to learn the unknown parameter $\\rho$ using the EM-algorithm. Denote the missing observation by $Z$ and derive the E-step of the algorithm, i.e., __(a)__ write the complete data log-likelihood $\\ell(\\rho)$, __(b)__ compute the posterior distribution of the missing observation, given the observed variables and current estimate for $\\rho$, and __(c)__ evaluate the expectation of $\\ell(\\rho)$ with respect to the posterior distribution of the missing observations.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "1. In general, for $X \\sim \\mathcal{N}_2(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, where $X=(X_1, X_2)^{T}$, $\\boldsymbol{\\mu}=(\\mu_1, \\mu_2)^{T}$ and $\\boldsymbol{\\Sigma} = \\begin{pmatrix} \n",
    "            \\sigma_1^{2} & \\rho\\sigma_{1}\\sigma_{2} \\\\ \n",
    "            \\rho\\sigma_{1}\\sigma_{2} & \\sigma_2^{2} \n",
    "            \\end{pmatrix}$, \n",
    "we have \n",
    "$$ X_1 \\mid X_2 = x_2 \\sim \\mathcal{N}\\left(\\mu_1 + \\frac{\\sigma_1}{\\sigma_2}\\rho(x_2-\\mu_2), (1-\\rho^2)\\sigma_1^{2}\\right),$$  with $\\rho$ being the correlation coefficient.\n",
    "2. For evaluating the expectation of $\\ell(\\rho)$, you can make use of the following two rules: \n",
    "    - $\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2} = trace(\\boldsymbol{\\Sigma}^{-1}\\mathbf{x_2x_2^T}).$\n",
    "    - if $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ then $\\langle{X^2}\\rangle = \\mu^2 + \\sigma^2$.\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Problem 1\n",
    "Write your solution to Problem 1 in LateX or attach a picture here. You can add a picture using the command ```!(imagename_in_folder.jpg)```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "$$\\ell(p)=log\\prod_{i=1}^{3}p(x_{i}|p)=\\sum_{i=1}^{3}log\\;p(x_{i}|p)=\\sum_{i=1}^{3}\\mathcal{N}_{2}(x_{i}|0,\\textstyle\\sum)$$\n",
    "\n",
    "$$\\sum_{i=1}^{3}\\frac{1}{\\sqrt{2\\pi det\\textstyle\\sum}}e^{-\\frac{1}{2}(x_{i}-\\bar{x_{i}})^{T}\\textstyle\\sum_{i}^{-1}(x_{i}-\\bar{x_{i}})}$$\n",
    "\n",
    "$$-\\frac{1}{2}x_{1}^{T}\\textstyle\\sum^{-1}x_{1}-\\frac{1}{2}x_{2}^{T}\\textstyle\\sum^{-1}x_{2}-\\frac{1}{2}x_{3}^{T}\\textstyle\\sum^{-1}x_{3}-\\frac{3}{2}log(1-\\rho)^{2}+constant$$\n",
    "\n",
    "**(b)**\n",
    "\n",
    "By using the given formula: \n",
    "\n",
    "$$X_1 \\mid X_2 = x_2 \\sim \\mathcal{N}\\left(\\mu_1 + \\frac{\\sigma_1}{\\sigma_2}\\rho(x_2-\\mu_2), (1-\\rho^2)\\sigma_1^{2}\\right)$$\n",
    "\n",
    "And recalling that $X_{i}\\sim \\mathcal{N}_{2}(0,\\Sigma)$ and that $\\Sigma = \\begin{bmatrix} 1 & \\rho\\\\ \\rho & 1 \\end{bmatrix}$, we can find that:\n",
    "\n",
    "$$p(Z|X_{22})=\\mathcal{N}\\left(Z|0+\\frac{1}{1}\\rho(x_{22}-0),(1-\\rho^{2})1^{2}\\right)$$\n",
    "\n",
    "$$\\mathcal{N}\\left(Z|\\rho x_{22},(1-\\rho^{2})\\right)$$\n",
    "\n",
    "$$E(Z)=\\rho x_{22}, E(Z^{2})=\\rho^{2}x_{22}(1-p)^{2}$$\n",
    "\n",
    "**(c)**\n",
    "\n",
    "$$Q(\\rho,\\rho_{0})\\equiv E_{Z|x,\\rho_{0}}log\\;\\ell(p)$$\n",
    "\n",
    "$$E\\left(-\\frac{1}{2}x_{1}^{T}\\textstyle\\sum^{-1}x_{1}-\\frac{1}{2}x_{2}^{T}\\textstyle\\sum^{-1}x_{2}-\\frac{1}{2}x_{3}^{T}\\textstyle\\sum^{-1}x_{3}-\\frac{3}{2}log(1-\\rho)^{2}+constant\\right)$$\n",
    "\n",
    "Where \n",
    "\n",
    "$$\\left(-\\frac{1}{2}x_{2}^{T}\\textstyle\\sum^{-1}x_{2}+ constant\\right)$$ \n",
    "\n",
    "$$E\\left(trace\\left(-\\frac{1}{2}\\textstyle\\sum^{-1}x_{2}x_{2}^{T}\\right)+ constant\\right)$$\n",
    "\n",
    "$$-\\frac{1}{2}trace\\left(\\textstyle\\sum^{-1}\\begin{bmatrix} \\sum_{xx} & \\sum_{xy}\\\\ \\sum{yx} & \\sum_{yy} \\end{bmatrix}+ constant\\right)$$\n",
    "\n",
    "$$ -\\frac{1}{2}trace\\left(\\textstyle\\sum^{-1}\\begin{bmatrix} E(Z^{2}) & E(Z)x_{22}\\\\ x_{22}E(Z) & x_{22}^{2} \\end{bmatrix}+ constant\\right)$$\n",
    "\n",
    "$$ -\\frac{1}{2}trace\\left(\\textstyle\\sum^{-1}\\begin{bmatrix} \\rho^{2}x_{22}(1-p)^{2} & \\rho x_{22}\\\\ \\rho x_{22} & x_{22}^{2} \\end{bmatrix}+ constant\\right)$$\n",
    "\n",
    "$$ -\\frac{1}{2}trace\\left(\\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}^{-1} \\begin{bmatrix} \\rho^{2}x_{22}(1-p)^{2} & \\rho x_{22}\\\\ \\rho x_{22} & x_{22}^{2} \\end{bmatrix}+ constant\\right)$$\n",
    "\n",
    "$$ -\\frac{1}{2}trace\\left(\\frac{1}{1\\times1 - (-\\rho)\\times(-\\rho)}\\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}\\begin{bmatrix} \\rho^{2}x_{22}(1-p)^{2} & \\rho x_{22}\\\\ \\rho x_{22} & x_{22}^{2} \\end{bmatrix}+ constant\\right)$$\n",
    "\n",
    "$$ -\\frac{1}{2}trace\\left(\\frac{1}{1-p^{2}}\\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}\\begin{bmatrix} \\rho^{2}x_{22}(1-p)^{2} & \\rho x_{22}\\\\ \\rho x_{22} & x_{22}^{2} \\end{bmatrix}+ constant\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9800302ece58fdfd2d96bf8d25a31fce",
     "grade": false,
     "grade_id": "cell-621b0c5f1a67b456",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: Extension of 'simple example' from the lecture\n",
    "Suppose that we have $N$ independent observations $x = ( x_1, \\dots, x_N )$ from a two-component mixture of univariate Gaussian distributions with unknown mixing co-efficients and unknown mean of the second component:\n",
    "$$ p(x_{n} \\mid \\theta,\\tau)=(1-\\tau)\\mathcal{N}(x_{n}|0,1)+\\tau\\mathcal{N}(x_{n} \\mid \\theta,1).$$\n",
    "\n",
    "**(a)** Write down the complete data log-likelihood and derive the EM-algorithm for learning the maximum likelihood estimates for $\\theta$ and $\\tau$. \n",
    "\n",
    "**(b)** Simulate some data from the model ($N = 100$ samples) with the true values of parameters $\\theta$ = 3 and $\\tau = 0.5$. Run your EM algorithm to see whether the learned parameters converge close to the true values (by e.g. just listing the estimates from a few iterations or plotting them). Use the code template below (after the answer cell) as a starting point. \n",
    "\n",
    "**HINT**: The E and M steps for simple example.pdf from the lecture material looks as follows\n",
    "```Python\n",
    "\t# E-step: compute the responsibilities r2 for component 2\n",
    "\tr1_unnorm = scipy.stats.norm.pdf(x, 0, 1)\n",
    "\tr2_unnorm = scipy.stats.norm.pdf(x, theta_0, 1)\n",
    "\tr2 = r2_unnorm / (r1_unnorm + r2_unnorm)\n",
    "\t\n",
    "\t# M-step: compute the parameter value that maximizes\n",
    "\t# the expectation of the complete-data log-likelihood.\n",
    "\ttheta[it] = sum(r2 * x) / sum(r2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Problem 2(a)\n",
    "Write your solution to Problem 2(a) in LateX or attach a picture here. You can add a picture using the command ```!(imagename_in_folder.jpg)```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "The observation $x_{n}$ is generated from the first component with the probability $(1-\\tau)$ and from the second component with the probability of $\\tau$\n",
    "\n",
    "$$z\\sim Multinomial((1-\\tau),\\tau)^{T}$$\n",
    "\n",
    "$$x|z_{1}=(1,0)^{T} \\sim \\mathcal{N}(x_{n}|0,1)$$\n",
    "\n",
    "$$x|z_{2}=(0,1)^{T} \\sim \\mathcal{N}(x_{n}|\\theta,1)$$\n",
    "\n",
    "In the EM-algorithm we will maximize the expectation of the log-likelihood of the complete data (x,z):\n",
    "\n",
    "$$p(x, z|\\theta,\\tau) = log\\Bigg\\{\\prod_{n=1}^{N}p(x_{n},z_{n}|\\theta, \\tau)\\Bigg\\} = \\sum_{n=1}^{N}log\\;p(x_{n},z_{n}|\\theta, \\tau)$$\n",
    "\n",
    "$$\\sum_{n=1}^{N}[(1-\\tau)^{z_{n1}} \\times \\mathcal{N}(x_{n}|0,1)^{z_{n1}} \\times \\tau^{z_{n2}}\\times \\mathcal{N}(x_{n}|\\theta,1)^{z_{n2}}]$$\n",
    "\n",
    "$$\\sum_{n=1}^{N}\\big\\{z_{n1}log[\\mathcal{N}(x_{n}|0,1)] + z_{n2}log[\\mathcal{N}(x_{n}|\\theta,1)] + z_{n1}log(1-\\tau) + z_{n2}log\\tau\\big\\}$$\n",
    "\n",
    "$E-step 1^{0}:$ Compute the posterior distribution of the latent variables, given current estimates $\\theta_{0}$ and $\\tau_{0}$\n",
    "\n",
    "$$\\gamma(z_{n2})\\equiv p(z_{n2}=1|x_{n},\\theta_{0}, \\tau_{0})=\\frac{\\tau\\mathcal{N}(x_{n}|\\theta_{0},1)}{(1-\\tau_{0})\\mathcal{N}(x_{n}|0,1) + \\tau_{0}\\mathcal{N}(x_{n}|\\theta_{0},1)}$$\n",
    "\n",
    "$E-step 2^{0}:$ Evaluate the expectation of the complete data log-likelihood over the posterior distribution of the latent variables\n",
    "\n",
    "$$Q(\\theta,\\theta_{0},\\tau,\\tau_{0}) = E_{z|x,\\theta_{0},\\tau_{0}}[log\\;p(x,z|\\theta,\\tau)]$$\n",
    "\n",
    "$$\\sum_{n=1}^{N}\\big\\{E[z_{n1}log[\\mathcal{N}(x_{n}|0,1)]+E[z_{n2}log[\\mathcal{N}(x_{n}|\\theta,1)]+E[z_{n1}log(1-\\tau)] + E[z_{n2}log\\tau]\\big\\}$$\n",
    "\n",
    "$$\\sum_{n=1}^{N}\\big\\{[(1-\\gamma(z_{n1})]log[\\mathcal{N}(x_{n}|0,1)]+\\gamma(z_{n2})log[\\mathcal{N}(x_{n}|\\theta,1)]+[(1-\\gamma(z_{n1})]log(1-\\tau) + \\gamma(z_{n2})log\\tau\\big\\}$$\n",
    "\n",
    "$M-step - maximize Q(\\theta,\\theta_{0},\\tau,\\tau_{0})$ with respect to $\\theta$,$\\tau$ by taking the derivative with respect to (1) $\\theta$ and (2) $\\tau$\n",
    "\n",
    "$$\\;\\; \\frac{d}{d\\theta}Q(\\theta,\\theta_{0},\\tau,\\tau_{0}) = \\frac{d}{d\\theta}\\sum_{n=1}^{N}\\big\\{[(1-\\gamma(z_{n1})]log[\\mathcal{N}(x_{n}|0,1)]+\\gamma(z_{n2})log[\\mathcal{N}(x_{n}|\\theta,1)]+[(1-\\gamma(z_{n1})]log(1-\\tau) + \\gamma(z_{n2})log\\tau\\big\\}$$\n",
    "\n",
    "(1) Set $\\frac{d}{d\\theta}Q(\\theta,\\theta_{0},\\tau,\\tau_{0}) = 0$ we get:\n",
    "\n",
    "$$\\theta = \\frac{\\sum_{n=1}^{N}\\gamma(z_{n2})x_{n}}{\\sum_{n=1}^{N}\\gamma(z_{n2})}$$\n",
    "\n",
    "$$\\frac{1}{N_{2}}\\sum_{n=1}^{N}\\gamma(z_{n2})x_{n}$$\n",
    "\n",
    "Where $N_{2}$ is defined as $\\sum_{n=1}^{N}\\gamma(z_{n2})$, which is the effective number of observations assigned to component 2\n",
    "\n",
    "(2) Set $\\frac{d}{d\\tau}Q(\\theta,\\theta_{0},\\tau,\\tau_{0}) = 0$ we get:\n",
    "\n",
    "$$\\sum_{n=1}^{N}\\gamma(z_{n2})+\\tau\\lambda = N_{2}+\\tau\\lambda$$\n",
    "\n",
    "Taking into account that the mixing coefficients (probabilities) $\\tau$ sum up to 1: $\\sum_{n=1}^{N}\\tau_{n}=1$ and that $\\tau_{n}\\geq0\\;\\;\\forall\\;\\tau_{n}$, and by utilizing the Lagrange multiplier $\\lambda$, we eventually find that $\\lambda$ = $-N$. Thus,\n",
    "\n",
    "$$\\tau = \\frac{N_{2}}{N}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b9bb0b57a6fd5a083d6e41264dd3744",
     "grade": false,
     "grade_id": "cell-dc3663002fe794a6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta       tau\n",
      "1.0000000  0.1000000\n",
      "2.6157647  0.6743498\n",
      "3.1118685  0.5926984\n",
      "3.2068086  0.5654052\n",
      "3.2238128  0.5601163\n",
      "3.2268286  0.5591647\n",
      "3.2273626  0.5589957\n",
      "3.2274572  0.5589658\n",
      "3.2274739  0.5589605\n",
      "3.2274769  0.5589596\n",
      "3.2274774  0.5589594\n",
      "3.2274775  0.5589594\n",
      "3.2274775  0.5589594\n",
      "3.2274775  0.5589594\n",
      "3.2274775  0.5589594\n",
      "3.2274775  0.5589594\n",
      "3.2274775  0.5589594\n",
      "3.2274775  0.5589594\n",
      "3.2274775  0.5589594\n",
      "3.2274775  0.5589594\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# template for Problem 2(b)\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### Simulate data:\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "theta_true = 3\n",
    "tau_true = 0.5\n",
    "n_samples = 100\n",
    "\n",
    "x = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    # Sample from N(0,1) or N(theta_true,1)\n",
    "    if np.random.rand() < 1 - tau_true:\n",
    "        x[i] = np.random.normal(0, 1)\n",
    "    else:\n",
    "        x[i] = np.random.normal(theta_true, 1)\n",
    "\n",
    "\n",
    "### The EM algorithm:\n",
    "\n",
    "n_iter = 20\n",
    "theta = np.zeros(n_iter)\n",
    "tau = np.zeros(n_iter)\n",
    "\n",
    "# Initial guesses for theta and tau\n",
    "theta[0] = 1\n",
    "tau[0] = 0.1\n",
    "\n",
    "for it in range(1, n_iter):\n",
    "    # The current estimates for theta and tau,\n",
    "    # computed in the previous iteration\n",
    "    theta_0 = theta[it-1]\n",
    "    tau_0 = tau[it-1]\n",
    "\n",
    "    # E-step: compute the responsibilities r1 and r2\n",
    "    # r1 = ?\n",
    "    # r2 = ?\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    try:\n",
    "        r1_unnorm = scipy.stats.norm.pdf(x, 0, 1)\n",
    "        r2_unnorm = scipy.stats.norm.pdf(x, theta_0, 1)\n",
    "        ri = r1_unnorm / (r1_unnorm + r2_unnorm)\n",
    "        r2 = r2_unnorm / (r1_unnorm + r2_unnorm)\n",
    "    except:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # M-step: compute the parameter values that maximize\n",
    "    # the expectation of the complete-data log-likelihood.\n",
    "    # theta[it] = ?\n",
    "    # tau[it] = ?\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    try:\n",
    "        theta[it] = sum(r2 * x) / sum(r2)\n",
    "        tau[it] = sum(r2) / len(r2)\n",
    "    except:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "# Print and plot the values of theta and tau in each iteration\n",
    "print(\"theta       tau\")\n",
    "for theta_i, tau_i in zip(theta, tau):\n",
    "    print(\"{0:.7f}  {1:.7f}\".format(theta_i, tau_i))\n",
    "\n",
    "plt.plot(range(n_iter), theta, label = 'theta')\n",
    "plt.plot(range(n_iter), tau, label = 'tau')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98a829947a922dca215c9d6079432b4a",
     "grade": false,
     "grade_id": "cell-839a538f0b1683f9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Problem 3: PyTorch\n",
    "Go through the PyTorch tutorials in the three links and answer the questions given below\n",
    "\n",
    "1) What is PyTorch: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "\n",
    "2) Autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\n",
    "\n",
    "3) Linear regression with PyTorch: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/linear_regression/main.py\n",
    "\n",
    "__(a)__ What are PyTorch Tensors and how do you run a CPU tensor on GPU? \n",
    "\n",
    "\n",
    "__(b)__ What is Automatic differentiation and autograd? \n",
    "\n",
    "\n",
    "__(c)__ PyTorch constructs the computation graph dynamically as the operations are defined. In the 'linear regression with PyTorch' tutorial which line numbers indicates the completion of the computation graph, computation of the gradients and update of the weights, respectively? \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Solution to Problem 3\n",
    "Write your solution to Problem 3 here.\n",
    "\n",
    "**(a)**\n",
    "\n",
    "PyTorch Tensors are core building blocks of the Python-based scientific progamming package PyTorch. PyTorch Tensors are similar to NumPy's ndarrays with the exception that they can take benefit of parallel computation capabilities of GPU to improve computing. To run CPU tensor on GPU, one can move CPU tensors in and out of GPU by using ```torch.device``` object:\n",
    "\n",
    "```C++\n",
    "   device=torch.device('cuda'))\n",
    "   device=torch.device('cpu'))\n",
    "```\n",
    "\n",
    "Or more concisely:\n",
    "\n",
    "```C++\n",
    "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n",
    "\n",
    "**(b)**\n",
    "\n",
    "Autograd is a package that is central to all neural networks in Pytorch. The autograd package provides means for automatic differentation of arbitrary scalar valued functions executed either on CPU or GPU, such as loss functions. Autograd can be used for gradient computation with the tensor keyword ```requires_grad=True```. When this keyword is set for arbitraty number of tensors, autograd computes the sum of gradients of given tensors w.r.t. graph leaves by utilizing the chain rule. In case of any non-scalar tensors that require gradient computation, the autograd package computes the Jacobian-vector product. Furthermore, Autograd also creates an acyclic graph called the dynamic computational graph from all operations (functions) performed on gradient enabled tensors. \n",
    "\n",
    "**(c)**\n",
    "\n",
    "- Completion of the computation graph: lines _30-42_\n",
    "\n",
    "- Computation of the gradients: _line 41_\n",
    "\n",
    "- Update of the weights: _line 42_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
