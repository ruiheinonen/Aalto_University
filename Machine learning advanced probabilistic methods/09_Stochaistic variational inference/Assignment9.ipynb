{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d3756215cb8d119b0f4dec333009e26",
     "grade": false,
     "grade_id": "cell-648bd202808988e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2020)\n",
    "Pekka Marttinen, Santosh Hiremath, Marko Järvenpää, Tianyu Cui, Yogesh Kumar, Diego Mesquita, Zheyang Shen, Alexander Aushev, Khaoula El Mekkaoui, Joakim Järvinen.\n",
    "\n",
    "## Assignment 9, due on Tuesday, 31st March at 23:55.\n",
    "\n",
    "# SVI for linear regression using PyTorch\n",
    "\n",
    "In this exercise, we will see how to use stochastic variational inference (especially the pathwise estimator) to solve linear regression problem using autograd in PyTorch.\n",
    "\n",
    "### Bayesian Linear Regression\n",
    "The model is defined as follows: \n",
    "\\begin{align*}\n",
    "y_i &  \\sim \\mathcal{N}(w_0 + w_1x_i, \\sigma_l^2), \\quad x_i \\in \\mathbb{R}, \\sigma_l=5, i=1,\\ldots,N\\\\\n",
    "\\mathbf{w} &  \\sim\\mathcal{N}(0, \\alpha^2I).\n",
    "\\end{align*}\n",
    "Note: The data noise is large because the true model used to generate the data is more complex to which we are going to fit a linear model. \n",
    "\n",
    "Given data $\\mathcal{D} = \\{ (x_i, y_i) \\}_{i=1}^{N}$, we are interested in the posterior distribution $p(\\mathbf{w}|\\mathcal{D})$ which we approximate using  mean-field approximation: $$ p(\\mathbf{w}|\\mathcal{D}) \\approx q(\\mathbf{w}) = \\prod_{d=0}^1 q(w_d) = \\prod_{d=0}^1 \\mathcal{N}(w_d | \\mu_d,\\sigma_d^2)$$\n",
    "That is, we model each $w_d$ as an independent Gaussian with mean $\\mu_d$ and $\\sigma_d^2$ and use SVI to optimize them such that: \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\lambda} & = \\text{argmin}_{\\lambda}\\text{KL}[q(\\mathbf{w})|p(\\mathbf{w} | \\mathcal{D})] \\\\\n",
    "&= \\text{argmin}_{\\lambda} \\underbrace{\\mathbb{E}_{q_{\\lambda}(\\mathbf{w})}\\left[-\\log p(\\mathcal{D}|\\mathbf{w})\\right] + \\text{KL}\\left[q(\\mathbf{w})| p(\\mathbf{w})\\right]}_{Loss = - ELBO}+c. \n",
    "\\end{align}\n",
    "Here, the variational parameters are denotd by $\\lambda = \\{ (\\mu_d, \\sigma_d), i = 0, 1 \\}$. The first term of the ELBO is the expected log likelihood, which will be estimated using pathwise estimator and the second term is the KL between the approximate posterior $q_{\\lambda}(\\mathbf{w})$ and the prior $p(\\mathbf{w})$ that can be derived analytically in this case. \n",
    "We will solve this problem in three steps given as three problem below. In the first two problems we derive the two terms of the Loss which, in problem 3 are implemented using the pathwise estimator in PyTorch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84eaf30af7023fc974ba843c3bf84b7f",
     "grade": false,
     "grade_id": "cell-6a6b31a376b966c8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 1: Negative log-likelihood\n",
    "Write the negative log-likelihood (whose expectation is the first term in the Loss) as a scaled mean squared error. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Problem 1: \n",
    "Write your solution to Problem 1 in LateX or attach a picture here. You can add a picture using the command ```!(imagename_in_folder.jpg)```. \n",
    "\n",
    "ELBO is a function of variational parameters, therefore ELBO for linear regression model can be written as:\n",
    "\n",
    "$$\\mathcal{L}(q)=\\int q(w|\\lambda) log\\frac{p(w,x,y)}{q(w|\\lambda)}$$\n",
    "\n",
    "where $w$ is a vector of two parameters $w_{0}\\; and\\; w_{1}$ and where $p(w,x,y)$ can be written as $p(y|x,w)p(x)p(w)$, which gives:\n",
    "\n",
    "$$\\int q(w|\\lambda)\\log p(y|x,w)dw + \\int q(w|\\lambda)\\log\\frac{p(w)}{p(w|\\lambda)}dw +constant $$\n",
    "\n",
    "where the first term is the expectation and the second term is the negative Kullback-Leibler divergence, which gives:\n",
    "\n",
    "$$E_{q(w|\\lambda)}[\\log p(y|x,w)] + KL(q|\\lambda)|p(w)) + constant $$\n",
    "\n",
    "$$Loss(\\lambda) = -\\mathcal{L}(\\lambda) = E_{q(w|\\lambda)}[-\\log p(y|x,w)] +KL(q|\\lambda)|p(w)) + constant$$\n",
    "\n",
    "where the first term is the negative log likelihood cost and the second term is the complexity cost.\n",
    "\n",
    "Taking a step back, I show that since $y_i\\sim\\mathcal{N}(w_0 + w_1x_i,\\sigma_l^{2})$ and $\\mathbf{w}\\sim\\mathcal{N}(0, \\alpha^2I)$ we end up to the negative log likelihood cost $E_{q(w|\\lambda)}[-\\log p(y|x,w)]$ (given above) by taking advantage of the properties of the normal distribution:\n",
    "\n",
    "First (1) we maximize the product of normal distribution for each target value assuming data is i.i.d \n",
    "\n",
    "Second (2) we simplify the formulae by taking the log likelihood\n",
    "\n",
    "Third (3) since maximizing a function is the same as minimizing the negative of that function we, calculate the negative log likelihood\n",
    "\n",
    "Fourth (4) since the variance term $\\sigma^{2}$ does not depend on $y$, we can simply ignore it\n",
    "\n",
    "Fifth (5) lastly, we scale the loss with $\\frac{1}{n}$, that is the number of samples, which finally gives use the the negative log likelihood as a scaled mean-squared error\n",
    "\n",
    "(1)\n",
    "\n",
    "$$\\mathcal{L}(y^{1}, y^{2} ... y^{d} |x^{1}, x^{2} ... x^{d})$$\n",
    "$$= \\prod_{d=1}^{D}\\mathcal{N}(w_{d}|\\mu_{d},\\sigma_{d}^{2})$$\n",
    "\n",
    "$$=\\prod_{d=1}^{D}\\sqrt{\\frac{1}{2\\pi\\sigma^{2}}}\\times e^{-\\frac{1}{2\\sigma^{2}}}(y_{d}-\\hat{y_{d}})^{2}$$\n",
    "\n",
    "(2)\n",
    "\n",
    "$$\\log\\left(\\mathcal{L}(y^{1}, y^{2} ... y^{d} |x^{1}, x^{2} ... x^{d})\\right)$$\n",
    "\n",
    "$$=\\log\\left(\\prod_{d=1}^{D}\\sqrt{\\frac{1}{2\\pi\\sigma^{2}}}\\times e^{-\\frac{1}{2\\sigma^{2}}}(y_{d}-\\hat{y_{d}})^{2}\\right)$$\n",
    "\n",
    "$$=\\sum_{d=1}^{D}\\log\\left(\\sqrt{\\frac{1}{2\\pi\\sigma^{2}}}\\right)\\times \\log\\left(e^{-\\frac{1}{2\\sigma^{2}}}(y_{d}-\\hat{y_{d}})^{2}\\right)$$\n",
    "\n",
    "$$=\\sum_{d=1}^{D}\\log\\left(\\sqrt{\\frac{1}{2\\pi\\sigma^{2}}}\\right)\\times \\log\\left(-\\frac{1}{2\\sigma^{2}}(y_{d}-\\hat{y_{d}})^{2}\\right)$$\n",
    "\n",
    "(3)\n",
    "\n",
    "$$-\\log\\left(\\mathcal{L}(y^{1}, y^{2} ... y^{d} |x^{1}, x^{2} ... x^{d})\\right)$$\n",
    "\n",
    "$$ = -\\sum_{d=1}^{D}-\\frac{1}{2\\sigma^{2}}(y_{d}-\\hat{y_{d}})^{2}$$\n",
    "\n",
    "$$ = \\sum_{d=1}^{D}\\frac{1}{2\\sigma^{2}}(y_{d}-\\hat{y_{d}})^{2}$$\n",
    "\n",
    "(4)\n",
    "\n",
    "$$\\sum_{d=1}^{D}\\frac{1}{2}(y_{d}-\\hat{y_{d}})^{2}$$\n",
    "\n",
    "$$=\\frac{1}{2}\\sum_{d=1}^{D}(y_{d}-\\hat{y_{d}})^{2}$$\n",
    "\n",
    "(5)\n",
    "\n",
    "$$\\frac{1}{m}\\log(\\mathcal{L}(y^{1} ... y^{2}, y^{n} |x^{1}, x^{2} ... x^{n})$$\n",
    "\n",
    "$$=\\frac{1}{2m}\\sum_{d=1}^{D}(y_{d}-\\hat{y_{d}})^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "478e5434aefadcef5470e1947ae94ab8",
     "grade": false,
     "grade_id": "cell-c4d2a5b39be37cd7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: Derive KL Divergence \n",
    "\n",
    "Derive the analytic solution of $\\text{KL}[q_{\\lambda}(\\mathbf{w})|p(\\mathbf{w})]$. This will be required in Problem 3.\n",
    "\n",
    "__Hint:__ Given $\\mathbf{w}$ is a MVN with diagonal covarience and the mean-field approximation of $q_{\\lambda}(\\mathbf{w})$, the KL divergence for both the components of $\\mathbf{w} = (w_0, w_1)$ will have the same form. So this reduces to deriving the KL between two univariate Guassians.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Problem 2: \n",
    "Write your solution to Problem 2 in LateX or attach a picture here. You can add a picture using the command ```!(imagename_in_folder.jpg)```. \n",
    "\n",
    "With the given hint we can write the KL divergence between two univariate gaussians as\n",
    "\n",
    "$$KL(p,q)=-\\int p(x)\\log q(x)dx +\\int p(x)\\log p(x)dx$$\n",
    "\n",
    "Where\n",
    "$$\\int p(x)\\log p(x)dx = -\\frac{1}{2}(1+\\log 2\\pi\\sigma_{1}^{2})$$\n",
    "\n",
    "And where\n",
    "$$-\\int p(x)\\log q(x)dx = -\\int p(x)\\log\\sqrt{\\frac{1}{2\\pi\\sigma_{2}^{2}}}\\times e^{-\\frac{(x-\\mu_{2})^{2}}{2\\sigma_{2}^{2}}}dx$$\n",
    "\n",
    "Which we can rewrite as\n",
    "\n",
    "$$\\frac{1}{2}\\log 2\\pi\\sigma_{1}^{2}-\\int p(x)\\log e^{-\\frac{(x-\\mu_{2})^{2}}{2\\sigma_{2}^{2}}}dx$$\n",
    "\n",
    "Taking the logarithm\n",
    "\n",
    "$$\\frac{1}{2}\\log 2\\pi\\sigma_{1}^{2}-\\int p(x)\\left(-\\frac{(x-\\mu_{2})^{2}}{2\\sigma_{2}^{2}}\\right)dx$$\n",
    "\n",
    "After separating the sums and taking $\\sigma_{2}^{2}$ out of the integral\n",
    "\n",
    "$$\\frac{1}{2}\\log 2\\pi\\sigma_{1}^{2} + \\frac{\\int p(x)x^{2}dx-\\int p(x)2x\\mu_{2}dx+\\int p(x)\\mu_{2}¨{2}dx}{2\\sigma_{2}^{2}}$$\n",
    "\n",
    "$$\\frac{1}{2}\\log 2\\pi\\sigma_{1}^{2} + \\frac{E(x^{2}-2E(x)\\mu_{2}+\\mu_{2}^{2}}{2\\sigma_{2}^{2}}$$\n",
    "\n",
    "We know that $var(x)$ = $E(x^{2})-E(x)^{2}$ so therefore $E(x^{2}) = \\sigma_{1}^{2} + \\mu_{1}^{2}$\n",
    "\n",
    "Applying that we get \n",
    "\n",
    "$$\\frac{1}{2}\\log 2\\pi\\sigma_{1}^{2} + \\frac{\\sigma_{1}^{2}+\\mu_{1}^{2}-2\\mu_{1}\\mu_{2}+\\mu_{2}^{2}}{2\\sigma_{2}^{2}}$$\n",
    "\n",
    "Which we can rewrite as\n",
    "\n",
    "$$\\frac{1}{2}\\log 2\\pi\\sigma_{1}^{2} + \\frac{\\sigma_{1}^{2}+(\\mu_{1}-\\mu_{2})^{2}}{2\\sigma_{2}^{2}}$$\n",
    "\n",
    "Putting the two terms $KL(p,q)=-\\int p(x)\\log q(x)dx +\\int p(x)\\log p(x)dx$ together we get\n",
    "\n",
    "$$\\frac{1}{2}\\log 2\\pi\\sigma_{1}^{2} + \\frac{\\sigma_{1}^{2}+(\\mu_{1}-\\mu_{2})^{2}}{2\\sigma_{2}^{2}} -\\frac{1}{2}(1+\\log 2\\pi\\sigma_{1}^{2})$$\n",
    "\n",
    "$$\\log\\frac{\\sigma_{2}}{\\sigma_{1}}+\\frac{\\sigma_{1}^{2}+(\\mu_{1}-\\mu_{2})^{2}}{2\\sigma_{2}^{2}} -\\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c587447c006b0c247b9ca3b6e3ca3cc",
     "grade": false,
     "grade_id": "cell-a2fefc01d5c88a7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 3: Pathwise Estimator in PyTorch\n",
    "Complete the code template below that implements the pathwise estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c2f6178c25d623dbedf84d84660aa1a",
     "grade": false,
     "grade_id": "cell-29a37732d95c26e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2c1b64e518>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Starter code for problem 3\n",
    "# We first simulate the data using following simulator to generate our training and test data: \n",
    "# $y_i=x_i+0.7\\sin(3x_i)+\\epsilon,$ where $\\epsilon\\sim\\mathcal{N}(0,0.16)$\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We define a function to generate the data according to the simulator\n",
    "def data_generation(num_data, interval):\n",
    "    x = np.random.rand(num_data,1) * (interval[1] - interval[0]) + interval[0]\n",
    "    e = np.random.randn(num_data,1) * 0.4\n",
    "    y =  x + 0.7 * np.sin(3 * x) + e\n",
    "    return torch.tensor(x, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "# Generate the 100 data points with x in [-3, 3] for training, validation, and test dataset.\n",
    "interval = [-3,3]\n",
    "num_data = 100\n",
    "x_train, y_train = data_generation(num_data, interval)\n",
    "x_val, y_val = data_generation(num_data, interval)\n",
    "x_test, y_test = data_generation(num_data, interval)\n",
    "# Visulize the data\n",
    "fig, ax = plt.subplots()\n",
    "x_plot = torch.linspace(-3., 3., 1000)\n",
    "y_plot = x_plot + 0.7 * torch.sin(3 * x_plot)\n",
    "ax.plot(x_train, y_train, '.')\n",
    "ax.plot(x_plot, y_plot, '-', color='red')\n",
    "\n",
    "ax.legend(('Simulated Datapoints','Simulator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41b22576882208864c33e871a644e607",
     "grade": false,
     "grade_id": "cell-f27332480038713c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9492], grad_fn=<AddBackward0>)\n",
      "tensor([-0.2352], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e3a62cdd8758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msigma_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-e3a62cdd8758>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(blr, x, y, x_test, y_test, sigma_l, learning_rate, batch_size, num_epoch)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# Forward passing for one mini-batch of data, and calculate the KL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstochastic_flag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# Exercise: Calculate the value of the loss, the negative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-e3a62cdd8758>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, stochastic_flag)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# forward pass for a mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# template for problem 3\n",
    "# We define a multivariate Bayesian linear regression model, which has input_dim features and output_dim outputs\n",
    "class linear_regression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, sigma = 1.):\n",
    "        super(linear_regression, self).__init__()\n",
    "        \n",
    "        # Define the input and output dimension of the LR model\n",
    "        # In this example, input_dim and output_dim are both 1;\n",
    "        # They can be other integers when this class is used as the Bayesian neural network layers \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # set standard deviation of the prior (the $\\sigma_w$)\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        scale = 1. * np.sqrt(6. / (input_dim + output_dim))        \n",
    "        # EXERCISE: Initialize the approximated posterior distribution over the weight and bias terms\n",
    "        # (i.e. specify values for the corresponding variational parameters).\n",
    "        # All the weights are assumed independent from each other.\n",
    "        # Initialize the mean parameters from a uniform distribution over (-scale, scale) to improve stability.\n",
    "        # Instead of parametrizing the standard deviation sigma directly, we parametrize it using rho:\n",
    "        # sigma = log(1 + exp(rho)) to keep it positive during training.\n",
    "        # This way we don't need to use a positivity constraint during optimization.\n",
    "        \n",
    "        self.mu_bias = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-scale, scale)) # given as example\n",
    "        # self.rho_bias = ?\n",
    "        # self.mu_weights = ?\n",
    "        # self.rho_weights = ?\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        try:\n",
    "            self.rho_bias = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-scale, scale))\n",
    "            self.mu_weights = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-scale, scale))\n",
    "            self.rho_weights = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-scale, scale))\n",
    "        except:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "    def forward(self, x, stochastic_flag):\n",
    "        eps = 1e-7 \n",
    "        \n",
    "        # Compute the standard deviation according to previous parametrization.\n",
    "        sigma_weights= torch.log(1 + torch.exp(self.rho_weights))\n",
    "        sigma_bias = torch.log(1 + torch.exp(self.rho_bias))\n",
    "        \n",
    "        if stochastic_flag:           \n",
    "            # stochastic forward pass during training\n",
    "            \n",
    "            # EXERCISE: Sample one set of weights from the current posterior approximation. \n",
    "            # These sampled weights will then be used to complete a forward pass for a mini-batch of data.\n",
    "            # Hints: you should first generate a sample from a standard normal \n",
    "            # distribution (epsilon-weights, epsilon-bias) and transform it to the\n",
    "            # posterior distribution (weights, bias) according to the posterior mean\n",
    "            # and variance (this is the 'reparametrization trick')\n",
    "            \n",
    "            epsilon_bias = torch.randn(self.output_dim) # shown as an example\n",
    "            # epsilon_weights = ?\n",
    "            # bias = ?\n",
    "            # weights = ? \n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            try:\n",
    "                epsilon_weights = torch.randn(self.output_dim)\n",
    "                #bias = torch.exp(0.5*sigma_bias)*epsilon_weights\n",
    "                bias = sigma_weights + epsilon_weights*sigma_bias\n",
    "                weights = self.mu_weights + epsilon_weights*self.mu_bias\n",
    "            except:\n",
    "                raise NotImplementedError()\n",
    "            \n",
    "            # forward pass for a mini-batch\n",
    "            output = torch.mm(x, weights) + bias\n",
    "            print(output)\n",
    "            \n",
    "        else:\n",
    "            # forward pass with the mean of posterior distribution during testing\n",
    "            output = torch.mm(x, self.mu_weights) + self.mu_bias\n",
    "            print(output)\n",
    "            \n",
    "        # calculate KL\n",
    "        # EXERCISE: calculate the KL divergence between the prior and the posterior        \n",
    "        # Hint: It is the solution you have computed in problem 1; the summation \n",
    "        # of the KL between two one dimensional Gaussian distributions\n",
    "        # KL_weights = ? \n",
    "        # KL_bias = ?\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        try:\n",
    "            # KL_weights = (sigma_bias+(weights[0], epsilon_weights[0])**2)/(2*sigma_bias**2)-1/2\n",
    "            KL_weights = torch.log(self.sigma)-torch.log(sigma_weights) + ((sigma_weights+(self.mu_weights-weights)**2)/(2*self.sigma**2))-0.5\n",
    "            KL_bias = torch.log(sigma_bias)-torch.log(bias) + ((sigma_weights+(self.mu_weights-weights)**2)/(2*sigma_bias**2))-0.5\n",
    "            #KL_weights = 1/(2*self.sigma)*(torch.log(sigma_weights)+(sigma_weights+self.mu_weights)**2)\n",
    "            #KL_bias = -1/2\n",
    "        except:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        KL = KL_weights + KL_bias             \n",
    "        return output, KL   \n",
    "\n",
    "def training(blr, x, y, x_test, y_test, sigma_l, learning_rate = 0.001, batch_size = 10, num_epoch=100):\n",
    "    \n",
    "    # Set the parameters that you want to optimize during training\n",
    "    parameters = set(blr.parameters())\n",
    "    \n",
    "    # We use Adam to do optimization, with learning rate equals to learning_rate, eps is used to stablize the training\n",
    "    optimizer = optim.Adam(parameters, lr = learning_rate, eps=1e-3)\n",
    "    \n",
    "    # We use MSE loss since it's a regression problem\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "\n",
    "    num_data, num_dim = x.shape\n",
    "    y = y.view(-1, 1)\n",
    "    data = torch.cat((x, y), 1)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        # We permute the data for each epoch to decorrelate the training process\n",
    "        data_perm = data[torch.randperm(len(data))]\n",
    "        x = data_perm[:, 0:-1]\n",
    "        y = data_perm[:, -1]\n",
    "    \n",
    "        for index in range(int(num_data/batch_size)):\n",
    "            inputs = x[index*batch_size : (index+1)*batch_size]\n",
    "            labels = y[index*batch_size : (index+1)*batch_size].view(-1,1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward passing for one mini-batch of data, and calculate the KL\n",
    "            output, kl = blr(inputs, stochastic_flag=True)\n",
    "            output.shape\n",
    "            # Exercise: Calculate the value of the loss, the negative \n",
    "            # ELBO, from the outputs of the linear regression model (output, kl)\n",
    "            # Hint: the expected negative log-likelihood can be estimated by the MSE \n",
    "            # divided by (2*variance) for Gaussian likelihood functions (allowing \n",
    "            # you to use the 'criterion' defined above).\n",
    "            \n",
    "            # loss = ?\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            try:\n",
    "                #loss = torch.sum(criterion(output, labels) / (2*sigma_1**2)+torch.mean(torch.log(sigma_1))).view(-1,1)\n",
    "                loss = criterion(output, labels)/(2*sigma_l**2)\n",
    "            except:\n",
    "                raise NotImplementedError()\n",
    "            \n",
    "            # backpropogate the gradient     \n",
    "            loss.backward()\n",
    "            # optimize with SGD\n",
    "            optimizer.step()\n",
    "            \n",
    "        # calculate the training loss after one epoach \n",
    "        output_x, _= blr(x, stochastic_flag = False)\n",
    "        train_errors.append(criterion(output_x, y.view(-1,1)))\n",
    "        \n",
    "        # calculate the validation loss after one epoach \n",
    "        output_x_test, _ = blr(x_test, stochastic_flag = False)\n",
    "        val_errors.append(criterion(output_x_test, y_test.view(-1,1)))\n",
    "\n",
    "        if (epoch % 100) == 0:\n",
    "            print('EPOACH %d: TRAIN LOSS: %.4f; VAL LOSS IS: %.5f.'% (epoch+1, train_errors[epoch], val_errors[epoch]))        \n",
    "\n",
    "            \n",
    "# train the model \n",
    "num_input = 1; num_output = 1\n",
    "BLR = linear_regression(num_input, num_output)\n",
    "\n",
    "# Setting all the hyper-parameters\n",
    "learning_rate = 1e-2\n",
    "batch_size = 50; num_epoch = 500; sigma_l = 5\n",
    "training(BLR, x_train, y_train, x_val, y_val, sigma_l, learning_rate, batch_size, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b74d742a4b10fde4cca8982569cc890f",
     "grade": false,
     "grade_id": "cell-9798756984979c17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## test the trained BLR\n",
    "# We calculate the true values of x_plot\n",
    "x_plot = torch.linspace(-3., 3., 1000)\n",
    "y_plot = x_plot + 0.7 * torch.sin(3 * x_plot)\n",
    "\n",
    "# One benefit of being a Bayesian is that you can capture the predictive uncertainty: \n",
    "# Use the stochastic forward passing during prediction, and calculate the sample mean and \n",
    "# sample standard deviation of predictions for different sets of weights.\n",
    "\n",
    "iteration = 100;\n",
    "x_pred = []\n",
    "for i in range(iteration):\n",
    "    stochastic_flag = True\n",
    "    x_pred.append(BLR(x_plot.view(-1,1), stochastic_flag)[0].view(-1).tolist())\n",
    "x_pred = np.array(x_pred)\n",
    "\n",
    "# Calculate the mean and standard deviation of prediction according to the samples\n",
    "x_pred_mean = np.mean(x_pred, axis = 0)\n",
    "x_pred_std = np.std(x_pred, axis = 0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train, y_train, '.')\n",
    "ax.plot(x_plot, y_plot, '-', color='red')\n",
    "\n",
    "# Draw the mean of the prediction and also corresponding 95% crediable intervals.\n",
    "ax.plot(x_plot, x_pred_mean, '-', color = 'deepskyblue')\n",
    "ax.plot(x_plot, x_pred_mean - 2 * x_pred_std, '-', color = 'skyblue')\n",
    "ax.plot(x_plot, x_pred_mean + 2 * x_pred_std, '-', color = 'skyblue')\n",
    "\n",
    "ax.legend(('Simulated Datapoints','Simulator', 'Prediction Mean', '95% Prediction CI'))\n",
    "\n",
    "# We can see that Bayesian linear regression cannot fit the data perfectly, because the simulator \n",
    "# that generates the data is nonlinear. However, the 95% crediable interval covers the true target \n",
    "# nearly all the time (95%), which means we can still know the possible interval of the target \n",
    "# even the model is misspecified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a8c9401de05cbc42f8fd29d2e6aff71",
     "grade": true,
     "grade_id": "cell-da054e700b3780ac",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
